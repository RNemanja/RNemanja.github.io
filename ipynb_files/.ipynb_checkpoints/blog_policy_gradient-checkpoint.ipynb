{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient explained. Properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_NOTE: Most of blogs that try to explain RL approaches somehow either explain concepts on a very low level and/or just jump in to complicated **Deep** examples.\n",
    "Research papers usually just explain concepts on a high level with maybe some pseudocode which is not straightforward to implement.\n",
    "Code implementations on GitHub are usually made to be self-contained and are complicated/require a lot of time to actually find basic examples.\n",
    "These are the reasons why I decided to make a basic step-by-step introduction, as well as to make a reminder for myself. Also, I needed to practice converting EQUATIONS to CODE._\n",
    "\n",
    "The full python implementation of the notions presented here integrated to test in the OpenAI's gym environment, can be found on my GitHub https://github.com/nemanja-rakicevic/rl_implementations/tree/master/policy_gradient_basic ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy gradient approach intuition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the policy gradient approach, and in general policy search methods, is to find a policy $\\pi$, which is basically a function that maps some state observation information to the next action the agent should perform.\n",
    "\n",
    "A policy can be: \n",
    "- **deterministic** &nbsp;&nbsp;\n",
    "$\\large a = \\pi(s)$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    ": always gives a deterministic value of an action for a certain input state observation $s$.\n",
    "- **stochastic** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$\\large a\\sim\\pi( a | s )$ &nbsp;&nbsp;\n",
    ": gives a probability distribution over the possible actions based on the input state.\n",
    "\n",
    "\n",
    "Since the policy is a funcion, there are many ways to chose the type of this function and the parameters $\\large \\theta$ which shape it. Therefore, in order to define our policy function $\\large \\pi_{\\theta}(a|s)$, which will select the appropriate action in every state, we need to select the function family and find its optimal parameters.\n",
    "\n",
    "One way to measure the efficacy of a policy is to optimise some __cost function__.\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "J(\\pi_{\\theta}) = \\mathbb{E}[R(\\tau|\\pi_{\\theta})]\n",
    "$$\n",
    "\n",
    "In this case, the __cost funciton__ is defined as the __expected value of the returns along some trajectory obtained following a policy__. This trajectory is defined as $\\large \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, ... , s_T)$, a sequence of _state_, _action_ and _reward_ tuples, where $s_T$ is the __terminal state__.\n",
    "\n",
    "The __return function__ $\\large R(\\tau)$ takes the sequence of rewards from a trajectory $\\tau$ and returns one value. This can be either a cummulative sum, or more usually sum of discounted rewards \n",
    "$\n",
    "\\large\n",
    "R(\\tau) = \\sum_{i=0}^{T} \\gamma^{i}r_i\n",
    "$\n",
    "where $\\large\\gamma$ is a __discount factor__ that determines the influence of the rewards that occur further in the future.\n",
    "\n",
    "\n",
    "%TODO: add <span style=\"color:red; font-family:Georgia; font-size:2em;\">FOTO.</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the optimal parameters\n",
    "\n",
    "We have defined what we need to do, now we need to maximese the expected returns for our policy, by tweaking the policy parameters:\n",
    "$$\n",
    "\\Large\n",
    "\\theta^* = argmax_{\\theta}(J(\\pi_{\\theta}))\n",
    "$$\n",
    "\n",
    "This is done iteratively by updating the parameter values $\\large\\theta = \\theta + \\Delta\\theta$ where usually $\\large\\Delta\\theta = - \\alpha \\frac{\\partial J(\\pi_{\\theta})}{\\partial \\theta}$ with $\\alpha$ being the __learning rate__ and $\\frac{\\partial J(\\pi_{\\theta})}{\\partial \\theta}$ a derivative of the cost function with respect to the parameters. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving the cost function\n",
    "\n",
    "Here's the full representation of the cost function, according to the definition of expectation:\n",
    "$$\n",
    "\\Large\n",
    "J(\\pi_{\\theta}) \n",
    "= \\mathbb{E}[R(\\tau|\\pi_{\\theta})]\n",
    "= \\int^T p(\\tau|\\pi_\\theta)R(\\tau|\\pi_\\theta)d\\tau\n",
    "$$\n",
    "Where  $p(\\tau|\\pi_\\theta)$ is basically the probability of seeing a trajectory $\\tau$ conditioned on following the policy $\\pi_\\theta$, and it is calculated multiplying the probabilities of each step:\n",
    "\n",
    "$\n",
    "\\large\n",
    "p(\\tau|\\pi_\\theta) = p(s_0) \\prod_{t=1}^T p(s_{t+1}|s_t,a_t) \\pi(a_t|s_t)\n",
    "$\n",
    "\n",
    "where $p(s_{t+1}|s_t,a_t)$ is the __transition probability__ (i.e. the __MDP model__).\n",
    "\n",
    "\n",
    "The derivative of the cost funtion w.r.t. the parameters is then:\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\frac{\\partial J(\\pi_{\\theta})}{\\partial \\theta} = \\nabla_\\theta  J(\\pi_{\\theta})\n",
    "= \\nabla_\\theta  \\int p(\\tau|\\pi_\\theta)R(\\tau|\\pi_\\theta)d\\tau\n",
    "=   \\int \\nabla_\\theta p(\\tau|\\pi_\\theta)R(\\tau|\\pi_\\theta)d\\tau\n",
    "$\n",
    "\n",
    "Since this would require deriving a very long product, a logarithm derivative rule is applied where \n",
    "$\n",
    "\\large\n",
    "\\nabla \\log f(x) = \\frac{1}{f(x)}\\nabla f(x)\n",
    "$.\n",
    "In order to get this right-hand side form in equation above, we need to multiply it by $\\large \\frac{p(\\tau|\\pi_\\theta)}{p(\\tau|\\pi_\\theta)}$.\n",
    "\n",
    "Thus we have:\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\nabla_\\theta  J(\\pi_{\\theta})\n",
    "= \\int  p(\\tau|\\pi_\\theta) \\frac{\\nabla_\\theta p(\\tau|\\pi_\\theta)}{p(\\tau|\\pi_\\theta)} R(\\tau|\\pi_\\theta)d\\tau \n",
    "= \\int  p(\\tau|\\pi_\\theta) \\underline{ \\nabla_\\theta \\log  p(\\tau|\\pi_\\theta)  R(\\tau|\\pi_\\theta)} d\\tau \n",
    "$\n",
    "\n",
    "------\n",
    "\n",
    "What this actually tells us is that:\n",
    "\n",
    "* the gradient of the expected returns is the expectation of the gradient of the log of the probability of the returns times the returns (underlined part).\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "\\nabla_\\theta  J(\\pi_{\\theta})\n",
    "= \\mathbb{E}[ \\nabla_\\theta \\log  p(\\tau|\\pi_\\theta)  R(\\tau|\\pi_{\\theta})]\n",
    "$$\n",
    "\n",
    "* When we now develop $p(\\tau|\\pi_\\theta)$, we see that now there is no need to take derivatives of the product, but of the sum, which is much easier.\n",
    "$\n",
    "\\large\n",
    "p(\\tau|\\pi_\\theta) = p(s_0) \\prod_{t=1}^T p(s_{t+1}|s_t,a_t) \\pi(a_t|s_t)\n",
    "\\\\\n",
    "\\large\n",
    "\\log p(\\tau|\\pi_\\theta) = \\sum_{t=1}^T \\log \\pi(a_t|s_t) + const\n",
    "$\n",
    "\n",
    "\n",
    "* And its derivative becomes:\n",
    "$\n",
    "\\large\n",
    "\\nabla_\\theta \\log p(\\tau|\\pi_\\theta) = \\sum_{t=1}^T \\nabla_\\theta \\log \\pi(a_t|s_t)\n",
    "$\n",
    "\n",
    "------\n",
    "\n",
    "Finally, the gradient of the cost function can be calculated approximately by sampling, using the iterative formula over K trajectories:\n",
    "$$\n",
    "\\Large\n",
    "\\nabla_\\theta  J(\\pi_{\\theta})\n",
    "= \\frac{1}{K} \\sum_{i=0}^K  \\left [  \\sum_{t=1}^T \\nabla_\\theta \\log \\pi(a_t|s_t) \\right ] R(\\tau|\\pi_{\\theta}) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving the policy. Characteristic eligibility\n",
    "\n",
    "The remaining piece of the puzzle is the $\\large \\nabla_\\theta \\log \\pi(a_t|s_t)$, also called __Characteristic eligibility__.\n",
    "This is now dependent on a particular function type we select for our policy. Below are a couple of examples, their derivations and code implementations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Implementing the Gradient Updates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax policy\n",
    "\n",
    "The Softmax policy parameterisation as presented in the original REINFORCE paper by Williams, is derived here.\n",
    "\n",
    "This policy gives the probability for each of the discrete action dimensions, so that the actions can be sampled: \n",
    "\n",
    "$\\boxed{ \\Large a \\sim \\pi(a|s) = \\frac{e^{\\theta_a^T s}}{\\sum_{a'}e^{\\theta_{a'}^T s}} }$\n",
    "\n",
    "In order to get the optimal policy we need to find the optimal parameters $\\large \\theta$, through the gradient of the cost function, so we need to calculate the characteristic eligibility.\n",
    "Each column of the parameter matrix $\\large \\theta_x$ corresponds to the approriate action $\\large x$, and it's easier if we show them separately here:\n",
    "\n",
    "Using the property of the logarithm of a fraction is the nominator minus the denominator:\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\nabla_{\\theta_x} \\log \\left [\\frac{e^{\\theta_a^T s}}{\\sum_{a'}e^{\\theta_{a'}^T s}}\\right ]\n",
    "\\\\\n",
    "\\Large\n",
    "= \\nabla_{\\theta_x} \\log e^{\\theta_a^T s} - \\nabla_{\\theta} \\log \\sum_{a'}e^{\\theta_{a'}^T s}\n",
    "$ \n",
    "\n",
    "Now, the first element is non-zero only if $\\large \\theta = \\theta_a$. Meaning, we are calculating the parameters corresposponding to a particular action separately, so this first element will contribute only to the parameter column corresponting to action __a__.\n",
    "\n",
    "$ \n",
    "\\Large   \n",
    "\\nabla_{\\theta_x} \\log e^{\\theta_a^T s} =  \n",
    "\\begin{cases}\n",
    "    s, & \\text{if } a = x\\\\\n",
    "    0,   & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "While the second part is further derived:\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\nabla_{\\theta_x} \\log \\sum_{a'}e^{\\theta_{a'}^T s}\n",
    "= \\frac{1}{\\sum_{a'}e^{\\theta_{a'}^T s}}  \\nabla_{\\theta} \\sum_{a'}e^{\\theta_{a'}^T s}\n",
    "= \\frac{ s \\cdot e^{\\theta_{a}^T s}}{\\sum_{a'}e^{\\theta_{a'}^T s}}\n",
    "= s \\cdot \\pi_{\\theta_x}(a_x | s)\n",
    "$\n",
    "\n",
    "Where the gradient of the sum is nonzero only if $\\large a=x$.\n",
    "\n",
    "Therefore, the updates will be:\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\text{if } \\theta = \\theta_x : s (1 - \\pi_{\\theta_x}(a_x | s) )\n",
    "\\\\\n",
    "\\Large\n",
    "\\text{if } \\theta \\neq \\theta_x :  - \\pi_{\\theta_x}(a_x | s)\n",
    "$\n",
    "\n",
    "In python code, it would look like this, where we implement the gradient updates in a matrix form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate characteristic eligibility\n",
    "common_deriv = np.tile(np.append(self.states[i], 1.), (self.num_action,1)) * self.action_probs[i].reshape(-1,1)\n",
    "# encode if x==a\n",
    "onehot = np.zeros((self.num_action,1))\n",
    "onehot[self.actions[i]] = 1\n",
    "# final matrix of derivatives\n",
    "deriv = np.tile(np.append(self.states[i], 1.), (self.num_action,1)) * onehot - common_deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the whole parameter update together in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updatePolicy_episodic(self):\n",
    "    \n",
    "    grad_pp = np.zeros_like(self.pp)  \n",
    "    \n",
    "    # Calculate episodic gradient update\n",
    "    num_iter = len(self.states) # episode length\n",
    "    for i in range(num_iter):\n",
    "        \n",
    "        # Calculate reward contribution with baseline\n",
    "        total_return = sum([self.gamma **i * rew for i, rew in enumerate(self.rewards[i:])])\n",
    "        advantage   = total_return - np.mean(self.rewards[i:])\n",
    "        \n",
    "        # Calculate characteristic eligibility\n",
    "        current_state = np.append(self.states[i], 1)\n",
    "        common_deriv = np.tile(current_state, (self.num_action,1)) * self.action_probs[i].reshape(-1,1)\n",
    "        onehot = np.zeros((self.num_action,1))\n",
    "        onehot[self.actions[i]] = 1\n",
    "        deriv = np.tile(current_state, (self.num_action,1)) * onehot - common_deriv\n",
    "        \n",
    "        # Calculate gradient\n",
    "        grad_pp += deriv * advantage\n",
    "    \n",
    "    # Normalise gradient updates over the episode\n",
    "    grad_pp /= num_iter\n",
    "    \n",
    "    # Update policy parameters\n",
    "    self.pp -= self.lr * grad_pp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Policy\n",
    "\n",
    "The Gaussian policy parameterisation as presented in the original REINFORCE paper by Williams, is derived here.\n",
    "\n",
    "The idea is to have a Gaussian function from which (continuous) actions can be sampled: \n",
    "\n",
    "$\\boxed{ \\Large a \\sim = \\pi(a|s) =\\mathcal{N}(\\mu, \\sigma)}$\n",
    "\n",
    "Usually, the $\\mu$ and $\\sigma$ parameters of the policy can be obtained directly as 2 regression outputs of a Neural Network. However, here another approach is applied, where 3 sets of parameters are learned:\n",
    "* Linear transformation parameters (like with softmax) - $\\large \\theta$\n",
    "* Gaussian's mean - $\\large \\mu$\n",
    "* Gaussian's covariance - $\\large \\sigma$\n",
    "\n",
    "The equation of the policy is given as in the paper as:\n",
    "$$\n",
    "\\Large\n",
    "\\pi(a | s,\\mu,\\sigma,\\theta) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(\\theta^T s - \\mu)^2}{2\\sigma^2}}\n",
    "$$\n",
    "\n",
    "Therefore we need to derive the update characteristic eligibility equations for each of the parameters in order to get the parameter updates at each step.\n",
    "\n",
    "### Parameter $\\theta$\n",
    "Using the property of the logarithm of a fraction is the nominator minus the denominator:\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\nabla_{\\theta} \\log \\left [\\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(\\theta^T s - \\mu)^2}{2\\sigma^2}} \\right ]\n",
    "\\\\\n",
    "\\Large\n",
    "= \\nabla_{\\theta} \\log e^{-\\frac{(\\theta^T s - \\mu)^2}{2\\sigma^2}} - \\nabla_{\\theta} \\log \\sigma\\sqrt{2\\pi}\n",
    "$ \n",
    "\n",
    "The log and exp cancel each other out, while second part does not cantain $\\theta$ so it's gradient is 0.\n",
    "\n",
    "$\n",
    "\\Large\n",
    "= \\nabla_{\\theta} \\left ( -\\frac{(\\theta^T s - \\mu)^2}{2\\sigma^2} \\right )- 0\n",
    "= -\\frac{1}{2\\sigma^2} 2 (\\theta^T s - \\mu) x\n",
    "= \\boxed{\\frac{\\mu - \\theta^T s}{\\sigma^2}x}\n",
    "$\n",
    "\n",
    "### Parameter $\\mu$\n",
    "Same property:\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\nabla_{\\mu} \\log \\left [\\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(\\theta^T s - \\mu)^2}{2\\sigma^2}} \\right ]\n",
    "\\\\\n",
    "\\Large\n",
    "= \\nabla_{\\mu} \\log e^{-\\frac{(\\theta^T s - \\mu)^2}{2\\sigma^2}} - \\nabla_{\\mu} \\log \\sigma\\sqrt{2\\pi}\n",
    "$ \n",
    "\n",
    "Again the log and exp cancel each other out, while second part does not cantain $\\theta$ so it's gradient is 0, and then just chain rule.\n",
    "\n",
    "$\n",
    "\\Large\n",
    "= \\nabla_{\\mu} \\left ( -\\frac{(\\theta^T s - \\mu)^2}{2\\sigma^2} \\right )- 0\n",
    "= -\\frac{1}{2\\sigma^2} 2 (\\theta^T s - \\mu) (-1)\n",
    "= \\boxed{\\frac{\\theta^T s - \\mu}{\\sigma^2}}\n",
    "$\n",
    "\n",
    "### Parameter $\\sigma$\n",
    "\n",
    "Again:\n",
    "\n",
    "$\n",
    "\\Large\n",
    "\\nabla_{\\sigma} \\log \\left [\\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(\\theta^T s - \\mu)^2}{2\\sigma^2}} \\right ]\n",
    "\\\\\n",
    "\\Large\n",
    "= \\nabla_{\\sigma} \\log e^{-\\frac{(\\theta^T s - \\mu)^2}{2\\sigma^2}} - \\nabla_{\\sigma} \\log \\sigma\\sqrt{2\\pi}\n",
    "$ \n",
    "\n",
    "Again the log and exp cancel each other out, and using the chain rule, the derivative of a fraction \n",
    "$\\left(\\frac{g}{h}\\right)' = \\frac{g'h - gh'}{h^2}$ and putting everything over a common denominator.\n",
    "The gradient of an expression which doesn't contain $\\sigma$ is 0.\n",
    "\n",
    "$\n",
    "\\Large\n",
    "= \\nabla_{\\sigma} \\left ( -\\frac{(\\theta^T s - \\mu)^2}{2\\sigma^2} \\right ) - \\frac{1}{\\sigma\\sqrt{2\\pi}} \\sqrt{2\\pi}\n",
    "\\\\\n",
    "\\Large\n",
    "= -\\frac{ \\nabla_{\\sigma}(\\theta^T s - \\mu)^2\\sigma - (\\theta^T s - \\mu)^2 \\nabla_{\\sigma}(2\\sigma^2)}{4\\sigma^4} - \\frac{1}{\\sigma}\n",
    "= -\\frac{ 0 - (\\theta^T s - \\mu)^2 4\\sigma}{4\\sigma^4} - \\frac{1}{\\sigma}\n",
    "= \\boxed{\\frac{(\\theta^T s - \\mu)^2 - \\sigma^2}{\\sigma^3}}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code. Putting all together\n",
    "\n",
    "Below is the python implementation of the equations above. \n",
    "\n",
    "The parameters are updated after each episode, and the for loop is over every step in the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updatePolicy_episodic(self):\n",
    "    \n",
    "    grad_pp0 = np.zeros_like(self.pp[0]) # theta\n",
    "    grad_pp1 = np.zeros_like(self.pp[1]) # mu \n",
    "    grad_pp2 = np.zeros_like(self.pp[2]) # sigma\n",
    "    \n",
    "    # Calculate episodic gradient update\n",
    "    num_iter = len(self.states) # episode length\n",
    "    for i in range(num_iter):\n",
    "        \n",
    "        # Calculate reward contribution with baseline\n",
    "        total_return = sum([self.gamma **i * rew for i, rew in enumerate(self.rewards[i:])])\n",
    "        advantage    = total_return - np.mean(self.rewards[i:])\n",
    "\n",
    "        # Calculate characteristic eligibility \n",
    "        current_state = np.append(self.states[i], 1)\n",
    "        y = np.dot(self.pp[0].T, current_state)\n",
    "\n",
    "        # parameter linear\n",
    "        eligib_0 = ((self.pp[1] - y)/self.pp[2]**2) * np.tile(current_state.reshape(-1,1), self.num_action)\n",
    "        grad_pp0 += eligib_0 * advantage\n",
    "        # parameter mean\n",
    "        eligib1 = (y - self.pp[1])/self.pp[2]**2\n",
    "        grad_pp1 += eligib1 * advantage\n",
    "        # parameter sigma\n",
    "        eligib2 = ((y - self.pp[1])**2 - self.pp[2]**2)/self.pp[2]**3\n",
    "        grad_pp2 += eligib2 * advantage\n",
    "        \n",
    "    # Normalise gradient updates over the episode\n",
    "    grad_pp0 /= num_iter\n",
    "    grad_pp1 /= num_iter\n",
    "    grad_pp2 /= num_iter\n",
    "    \n",
    "    # Update parameters\n",
    "    self.pp[0] -= self.lr * grad_pp0\n",
    "    self.pp[1] -= self.lr*self.pp[2]**2 * grad_pp1\n",
    "    self.pp[2] -= self.lr*self.pp[2]**2 * grad_pp2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
